{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL/ELT (Snowflake and Python) Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step 1:\n",
    "Extract and load the 41 comma delimited purchases data files and form a single table of purchases data; \n",
    "This query takes data from the directory and loads it into the staging area\n",
    "From the staging area, it loads data into the base tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating table structure\n",
    "\n",
    "cs.execute(\n",
    "    \"CREATE OR REPLACE TABLE po_table2(\" \\\n",
    "    \"PurchaseOrderID INT, SupplierID INT,\" \\\n",
    "    \"OrderDate date, DeliveryMethodID INT,\" \\\n",
    "    \"ContactPersonID INT, ExpectedDeliveryDate date,\" \\\n",
    "    \"SupplierReference STRING, IsOrderFinalized INT,\" \\\n",
    "    \"Comments STRING, InternalComments STRING,\" \\\n",
    "    \"LastEditedBy INT, LastEditedWhen STRING,\" \\\n",
    "    \"PurchaseOrderLineID INT, StockItemID INT,\" \\\n",
    "    \"OrderedOuters INT, Description STRING,\" \\\n",
    "    \"ReceivedOuters INT, PackageTypeID INT,\" \\\n",
    "    \"ExpectedUnitPricePerOuter FLOAT, LastReceiptDate date,\" \\\n",
    "    \"IsOrderLineFinalized INT, Right_LastEditedBy INT,\" \\\n",
    "    \"Right_LastEditedWhen STRING)\"\n",
    ")\n",
    "\n",
    "import glob\n",
    "import snowflake.connector\n",
    "\n",
    "# Establish Snowflake Connection\n",
    "conn = snowflake.connector.connect(\n",
    "    user='Lynn',\n",
    "    password='Lynn961108li',\n",
    "    account='mzniiiw-cjb77811',\n",
    "    warehouse='MY_FIRST_WAREHOUSE',\n",
    "    database='TESTDB',\n",
    "    schema='TESTSCHEMA'\n",
    ")\n",
    "\n",
    "# Create a Cursor Object\n",
    "cs = conn.cursor()\n",
    "\n",
    "# Define the path and filename pattern\n",
    "folder_path = \"/Users/lynnli/Desktop/CaseData/Monthly_PO_Data\"\n",
    "\n",
    "# Upload CSV Files to PO_STAGE\n",
    "for year in range(2013, 2017):\n",
    "    file_pattern = f\"{year}-*\"\n",
    "    file_path_pattern = f\"{folder_path}/{file_pattern}.csv\"\n",
    "    for file_path in glob.glob(file_path_pattern):\n",
    "        try:\n",
    "            put_command = f\"PUT file://{file_path} @PO_STAGE1\"\n",
    "            cs.execute(put_command)\n",
    "        except Exception as e:\n",
    "            print(f\"Error while executing PUT command for file {file_path}: {e}\")\n",
    "\n",
    "\n",
    "# Command to copy data from stage to table. The below code verifies the number of rows successfully copied into that table\n",
    "# Run COPY INTO command with SKIP_HEADER = 1 to skip the header row\n",
    "try:\n",
    "    copy_into_command = \"\"\"\n",
    "        COPY INTO PO_TABLE2\n",
    "        FROM @PO_STAGE1\n",
    "        FILE_FORMAT = (TYPE = 'CSV' SKIP_HEADER = 1)\n",
    "        ON_ERROR = 'CONTINUE'\n",
    "    \"\"\"\n",
    "    cs.execute(copy_into_command)\n",
    "    check_data_command = \"SELECT COUNT(*) FROM PO_TABLE2\"\n",
    "    cs.execute(check_data_command)\n",
    "    row = cs.fetchone()\n",
    "    print(f\"Number of rows in po_table2: {row[0]}\")\n",
    "    print(\"Data successfully loaded into po_table2.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while executing COPY INTO command: {e}\")\n",
    "\n",
    "# To drop the columns with null values\n",
    "cs.execute(\"alter table PO_TABLE2 Drop column comments,internalcomments\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2:\n",
    "Create a calculated field that shows purchase order totals, i.e., for each order, sum the line item amounts \n",
    "(defined as ReceivedOuters * ExpectedUnitPricePerOuter), and name this field POAmount\n",
    "Execute the SQL query to create a new table with POAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    cs.execute(\"\"\"\n",
    "        CREATE TABLE new_po_table AS\n",
    "        SELECT *, \n",
    "               SUM(ReceivedOuters * ExpectedUnitPricePerOuter) OVER (PARTITION BY PurchaseOrderID) AS POAmount\n",
    "        FROM po_table2;\n",
    "    \"\"\")\n",
    "    print(\"New table 'new_po_table' with POAmount created.\") \n",
    "#Created a new table 'new_po_table' with POAmount\n",
    "except Exception as e:\n",
    "    print(f\"Error while executing SQL query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "STEP 3:\n",
    "Extract and load the supplier invoice XML data\n",
    "a. shred the data into a table (preferably in the COPY INTO process) where each row corresponds to a single invoice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using INSERT INTO\n",
    "\n",
    "# Extract and load the supplier invoice XML data\n",
    "for row in root.findall('row'):\n",
    "    try:\n",
    "        # Extract fields from the XML\n",
    "        SupplierTransactionID = int(row.find('SupplierTransactionID').text)\n",
    "        SupplierID = int(row.find('SupplierID').text)\n",
    "        \n",
    "        TransactionTypeID_element = row.find('TransactionTypeID')\n",
    "        TransactionTypeID = int(TransactionTypeID_element.text) if TransactionTypeID_element is not None else None\n",
    "        \n",
    "        PurchaseOrderID_element = row.find('PurchaseOrderID')\n",
    "        PurchaseOrderID = int(PurchaseOrderID_element.text) if PurchaseOrderID_element is not None and PurchaseOrderID_element.text is not None else None\n",
    "        \n",
    "        PaymentMethodID_element = row.find('PaymentMethodID')\n",
    "        PaymentMethodID = int(PaymentMethodID_element.text) if PaymentMethodID_element is not None and PaymentMethodID_element.text is not None else None\n",
    "        \n",
    "        SupplierInvoiceNumber_element = row.find('SupplierInvoiceNumber')\n",
    "        SupplierInvoiceNumber = int(SupplierInvoiceNumber_element.text) if SupplierInvoiceNumber_element is not None and SupplierInvoiceNumber_element.text is not None else None\n",
    "        \n",
    "        TransactionDate = row.find('TransactionDate').text\n",
    "        AmountExcludingTax = float(row.find('AmountExcludingTax').text)\n",
    "        TaxAmount = float(row.find('TaxAmount').text)\n",
    "        TransactionAmount = float(row.find('TransactionAmount').text)\n",
    "        OutstandingBalance = float(row.find('OutstandingBalance').text)\n",
    "        \n",
    "        FinalizationDate_element = row.find('FinalizationDate')\n",
    "        FinalizationDate = FinalizationDate_element.text if FinalizationDate_element.text else None\n",
    "\n",
    "        IsFinalized = int(row.find('IsFinalized').text)\n",
    "        LastEditedBy = int(row.find('LastEditedBy').text)\n",
    "        LastEditedWhen = row.find('LastEditedWhen').text\n",
    "        \n",
    "        # Prepare the insert command\n",
    "        insert_command = \"\"\"\n",
    "            INSERT INTO supplier_invoice_xml\n",
    "            (SupplierTransactionID, SupplierID, TransactionTypeID, PurchaseOrderID, PaymentMethodID, SupplierInvoiceNumber, TransactionDate, AmountExcludingTax, TaxAmount, TransactionAmount, OutstandingBalance, FinalizationDate, IsFinalized, LastEditedBy, LastEditedWhen)\n",
    "            VALUES\n",
    "            (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare the values\n",
    "        values = (\n",
    "            SupplierTransactionID, SupplierID, TransactionTypeID, PurchaseOrderID,\n",
    "            PaymentMethodID, SupplierInvoiceNumber, TransactionDate,\n",
    "            AmountExcludingTax, TaxAmount, TransactionAmount, OutstandingBalance,\n",
    "            FinalizationDate, IsFinalized, LastEditedBy, LastEditedWhen\n",
    "        )\n",
    "        \n",
    "        # Execute the insert command using parameterized query\n",
    "        cs.execute(insert_command, values)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting row with SupplierTransactionID: {SupplierTransactionID}. Error: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have achieved this method Using COPY INTO as well. For reference, we have retained the code for both the methods.\n",
    "The second method (commented) is using COPY INTO and the first method is using INSERT INTO.\n",
    "insert into took more time than copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cs = conn.cursor()\n",
    "\n",
    "# import xml.etree.ElementTree as ET\n",
    "# import pandas as pd\n",
    "\n",
    "# tree = ET.parse('/Users/lynnli/Desktop/CaseData/SupplierTransactions.xml')\n",
    "# root = tree.getroot()\n",
    "\n",
    "# # Create a list of dictionaries for each invoice\n",
    "# invoices = []\n",
    "\n",
    "# for row in root.findall('row'):\n",
    "#     invoice = {}\n",
    "#     for child in row:\n",
    "#         invoice[child.tag] = child.text\n",
    "#     invoices.append(invoice)\n",
    "    \n",
    "# # Create a dataframe from the list of dictionaries\n",
    "# data_invoices = pd.DataFrame(invoices)\n",
    "\n",
    "#converting the dataframe into a csv file\n",
    "# data_invoices.to_csv(\"supplier_invoices.csv\", index=False)\n",
    "\n",
    "# # create supplier_invoice table with columns \n",
    "# cs.execute(\"\"\"CREATE OR REPLACE TABLE supplier_invoice_xml1(\n",
    "#     SupplierTransactionID INTEGER,\n",
    "#     SupplierID INTEGER,\n",
    "#     TransactionTypeID INTEGER,\n",
    "#     PurchaseOrderID INTEGER,\n",
    "#     PaymentMethodID INTEGER,\n",
    "#     SupplierInvoiceNumber INTEGER,\n",
    "#     TransactionDate DATE,\n",
    "#     AmountExcludingTax FLOAT,\n",
    "#     TaxAmount FLOAT,\n",
    "#     TransactionAmount FLOAT,\n",
    "#     OutstandingBalance FLOAT,\n",
    "#     FinalizationDate DATE,\n",
    "#     IsFinalized STRING,\n",
    "#     LastEditedBy INTEGER,\n",
    "#     LastEditedWhen STRING\n",
    "#     );\n",
    "#     \"\"\")\n",
    "\n",
    "# # put the csv file into the Stage on Snowflake\n",
    "\n",
    "# put_query1 = f\"PUT 'file://{'supplier_invoices.csv'}' @mystage3;\"\n",
    "# cs.execute(put_query1)\n",
    "\n",
    "# # Use copy into to copy file into the table \n",
    "\n",
    "# copy_into_query1 = f\"\"\"\n",
    "# COPY INTO supplier_invoice_xml1\n",
    "#          FROM (\n",
    "#              SELECT \n",
    "#                  $1::INTEGER AS SupplierTransactionID,\n",
    "#                  $2::INTEGER AS SupplierID,\n",
    "#                  $3::INTEGER AS TransactionTypeID,\n",
    "#                  $4::INTEGER AS PurchaseOrderID,\n",
    "#                  $5::INTEGER AS PaymentMethodID,\n",
    "#                  $6::INTEGER AS SupplierInvoiceNumber,\n",
    "#                  $7::DATE AS TransactionDate,\n",
    "#                  $8::FLOAT AS AmountExcludingTax,\n",
    "#                  $9::FLOAT AS TaxAmount,\n",
    "#                  $10::FLOAT AS TransactionAmount,\n",
    "#                  $11::FLOAT AS OutstandingBalance,\n",
    "#                  $12::DATE AS FinalizationDate,\n",
    "#                  $13::STRING AS IsFinalized,\n",
    "#                  $14::INTEGER AS LastEditedBy,\n",
    "#                  $15::STRING AS LastEditedWhen\n",
    "#              FROM @mystage3/{'supplier_invoices.csv'.split(\"/\")[-1]}\n",
    "#          )\n",
    "#          FILE_FORMAT = (TYPE = 'CSV' skip_header = 1)\n",
    "#          ON_ERROR = 'CONTINUE';  -- Or specify another error handling strategy\n",
    "#      \"\"\"\n",
    "# cs.execute(copy_into_query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " STEP 4 & 5:\n",
    "\n",
    "\n",
    " Using the joined data across purchases data from step 2 and the supplier invoices data from step 3 (including matching rows only),\n",
    " create a calculated field that shows the difference between AmountExcludingTax and POAmount, name this field invoiced_vs_quoted, \n",
    " and save the result as a materialized view named purchase_orders_and_invoices. In the below code, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have created a table with join instead of a materialized view.\n",
    "\n",
    "cs.execute(\"\"\"create  table purchase_orders_and_invoices as\n",
    "select  po.*,invoice.AmountExcludingTax as invoiceamount\n",
    ",(invoice.AmountExcludingTax- po.poamount) as invoiced_vs_quoted\n",
    "FROM \n",
    "NEW_PO_TABLE po,\n",
    "SUPPLIER_INVOICE_XML invoice\n",
    "\n",
    "where po.supplierid=invoice.supplierid\n",
    "and po.purchaseorderid=invoice.purchaseorderid\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 6:\n",
    "\n",
    "Extract the supplier_case data from postgres, do not import the data into Python, instead use Python to move the data \n",
    "from postgres to your local drive and then directly into a Snowflake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries used for connecting to PostgreSQL databases from Python.\n",
    "\n",
    "pip install psycopg2-binary snowflake-connector-python\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "# PostgreSQL database connection parameters\n",
    "pg_params = {\n",
    "    \"host\": \"127.0.0.1\",\n",
    "    \"database\": \"WestCoastImporters\",\n",
    "    \"user\": \"jovyan\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"port\":\"8765\",\n",
    "\n",
    "}\n",
    "# Connect to PostgreSQL\n",
    "conn = psycopg2.connect(**pg_params)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Run the the sql command in  PostgreSQL using the supplier_case file\n",
    "with open('/Users/lynnli/Desktop/CaseData/supplier_case.pgsql', 'r') as f:\n",
    "    cur.execute(f.read())\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Export data to a CSV on your local drive\n",
    "csv_file_path = \"/Users/lynnli/Desktop/CaseData/postgresoutput.csv\" # Path to your CSV file\n",
    "\n",
    "query = \"COPY supplier_case TO STDOUT WITH CSV HEADER\"\n",
    "with open(csv_file_path, 'w') as f:\n",
    "cur.copy_expert(query, f)\n",
    "\n",
    "# 6 a.Consider creating a Python function that can take a csv file path as input and then generate field definitions \n",
    "# (field names and datatypes based on the header and data types in the file) that can then be used in CREATE TABLE statement.\n",
    "\n",
    "# creating a function to generate the field definitions for the CREATE TABLE statement\n",
    "import csv\n",
    "\n",
    "def generate_field_definitions(file_path):\n",
    "    # Open the CSV file for reading\n",
    "    with open(file_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        \n",
    "        # Read the headers and the first data row\n",
    "        headers = next(reader)\n",
    "        first_row = next(reader)\n",
    "        \n",
    "        definitions = []\n",
    "        \n",
    "        # Loop through each column in the first data row\n",
    "        for header, value in zip(headers, first_row):\n",
    "            \n",
    "            # If the value is a whole number, infer the datatype as INTEGER\n",
    "            if value.isdigit():\n",
    "                datatype = \"INTEGER\"\n",
    "            \n",
    "            # If the value is a decimal number, infer the datatype as FLOAT\n",
    "            elif \".\" in value and value.replace(\".\", \"\").isdigit():\n",
    "                datatype = \"FLOAT\"\n",
    "            \n",
    "            # Otherwise, infer the datatype as TEXT\n",
    "            else:\n",
    "                datatype = \"TEXT\"\n",
    "            \n",
    "            # Add the column definition to the list\n",
    "            definitions.append(f\"{header} {datatype}\")\n",
    "        \n",
    "        # Return the column definitions as a comma-separated string\n",
    "        return \", \".join(definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 6 b.You need to use psycopg2 or a similar Python library to connect to the postgres database within Python, issue a command to postgres \n",
    " to have postgres save the supplier_case data to file, and then use cs.execute to move the file to an internal Snowflake stage and \n",
    " eventually into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your CSV file\n",
    "csv_file_path = \"/Users/lynnli/Desktop/CaseData/postgresoutput.csv\"\n",
    "\n",
    "# Create a stage and upload the CSV file\n",
    "#cs.execute(\"CREATE OR REPLACE STAGE my_stage\")\n",
    "cs.execute(f\"PUT file://{csv_file_path} @mystage3\")\n",
    "\n",
    "# Generate table schema from CSV and create the table\n",
    "table_schema = generate_field_definitions(csv_file_path)\n",
    "create_table_query = f\"CREATE OR REPLACE TABLE supplier_case ({table_schema})\"\n",
    "cs.execute(create_table_query)\n",
    "\n",
    "# Copy data from stage to the table\n",
    "\n",
    "copy_query = \"\"\"\n",
    "COPY INTO supplier_case\n",
    "FROM @mystage3/postgresoutput.csv\n",
    "FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '' SKIP_HEADER = 1);\n",
    "\"\"\"\n",
    "cs.execute(copy_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7: Snowflake Marketplace \n",
    "\n",
    "     \n",
    " A. The weather data does not contain zip codes but you can use the approach in \n",
    " https://towardsdatascience.com/noaa-weather-data-in-snowflake-free-20e90ee916ed to find weather stations closest to each \n",
    " zip code (only use one weather station per zip code). For this to work you need to find a data file with zip code – \n",
    " geo location mappings, e.g., from the US census (ZCTAs are fine to use);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step to copy csv file to snowflake\n",
    "\n",
    "import snowflake.connector\n",
    "\n",
    "# Snowflake connection parameters\n",
    "conn = snowflake.connector.connect(\n",
    "user='Lynn',\n",
    "password='Lynn961108li',\n",
    "account='mzniiiw-cjb77811',\n",
    "warehouse='MY_FIRST_WAREHOUSE',\n",
    "database='TESTDB',\n",
    "schema='TESTSCHEMA'\n",
    ")\n",
    "\n",
    "# Connect to Snowflake\n",
    "cs = conn.cursor()\n",
    "\n",
    "\n",
    "# copy csv file to snowflake\n",
    "\n",
    "csv_file_path = \"/Users/lynnli/Desktop/CaseData/uszip.csv\" # file with us zip code\n",
    "# Create a stage and upload the CSV file\n",
    "#cs.execute(\"CREATE OR REPLACE STAGE my_stage\")\n",
    "cs.execute(f\"PUT file://{csv_file_path} @mystage3\")\n",
    "\n",
    "\n",
    "#create table in snowflake for storing us zip code\n",
    "\n",
    "cs.execute(\n",
    "    \"CREATE OR REPLACE TABLE \"\n",
    "    \"US_ZIP_TABLE(zip INT PRIMARY KEY, lat FLOAT, lng FLOAT, city VARCHAR(255), state_id VARCHAR(10), state_name VARCHAR(255), zcta BOOLEAN, parent_zcta STRING, population INT, density FLOAT, county_fips INT, county_name VARCHAR(255), county_weights TEXT, county_names_all TEXT, county_fips_all TEXT, imprecise BOOLEAN, military BOOLEAN, timezone VARCHAR(50))\"\n",
    ")\n",
    "\n",
    "#   Copy data from stage to table\n",
    "\n",
    "cs.execute(f\"COPY INTO US_ZIP_TABLE FROM @mystage3/uszip.csv FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY = '\\\"' SKIP_HEADER = 1) ON_ERROR = 'CONTINUE'\")\n",
    "\n",
    "# dropping null columns\n",
    "cs.execute(\"ALTER TABLE US_ZIP_TABLE DROP COLUMN parent_zcta;\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.Create a materialized view named supplier_zip_code_weather that contains the unique zip codes (PostalPostalCode) from \n",
    " the supplier data, date, and daily high temperatures, i.e., the view should have three columns (zip code, date, and high temperature) \n",
    " and one row per day and unique supplier zip code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a view for the table US_ZIP_TABLE that join zip code \n",
    "# with supplier case table to get supplier latitude and longitude\n",
    "\n",
    "cs.execute(\"\"\"\n",
    "CREATE VIEW  supplier_zip_main AS\n",
    "SELECT DISTINCT PostalPostalCode as Zip_Code, LAT as Supplier_Latitude, LNG as Supplier_Longitude\n",
    "FROM supplier_case A\n",
    "JOIN US_ZIP_TABLE B\n",
    "ON A.POSTALPOSTALCODE = B.ZIP;\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Create a view for the table ENVIRONMENT_DATA_ATLAS.ENVIRONMENT.NOAACD2019R to get max temperature for each day from snowflake marketplace\n",
    "\n",
    "cs.execute(\"\"\"create view weather_main as\n",
    "\n",
    "SELECT MAX(\"Value\") AS \"MaxValue Of Daily Temparature\", CAST(\"Stations Latitude\" AS FLOAT) as Weather_latitude \n",
    ", CAST(\"Stations Longitude\" AS FLOAT) as Weather_longitude\n",
    ", \"Date\"\n",
    "FROM ENVIRONMENT_DATA_ATLAS.ENVIRONMENT.NOAACD2019R\n",
    "WHERE \"Country ISO Code\" = 'USA' \n",
    "AND \"Indicator Name\" = 'Maximum temperature (Fahrenheit)' \n",
    "AND \"Units\" = 'Fahrenheit'\n",
    "GROUP BY 4,2,3\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculating distance between supplier and weather station and ranking them based on \n",
    "  # distance and selecting the nearest weather station\n",
    "\n",
    "# Creating a view supplier_zip_code_weather_view instead of materialized view \n",
    "  #as it is not supported in snowflake\n",
    "\n",
    "cs.execute(\"\"\" CREATE VIEW supplier_zip_code_weather_view AS \n",
    "\n",
    "WITH Distance_Calculated AS (\n",
    "  SELECT \n",
    "      E.Weather_latitude AS Weather_latitude,\n",
    "      E.Weather_longitude AS Weather_longitude,\n",
    "      E.\"Date\" as Date,\n",
    "      E.\"MaxValue Of Daily Temparature\" as Value,\n",
    "      z.Supplier_Latitude AS Supplier_Latitude,\n",
    "      z.Supplier_Longitude AS Supplier_Longitude,\n",
    "      z.Zip_Code as zipcode,\n",
    "      ST_DISTANCE(\n",
    "        ST_POINT(E.Weather_longitude,E.Weather_latitude),\n",
    "        ST_POINT(z.Supplier_Longitude, z.Supplier_Latitude) \n",
    "      ) <50000 AS Distance\n",
    "    FROM weather_main AS E\n",
    "    CROSS JOIN supplier_zip_main AS z\n",
    "),\n",
    "ranked_stations AS (\n",
    "  SELECT *,\n",
    "         ROW_NUMBER() OVER (PARTITION BY zipcode ORDER BY Distance ASC) AS rank\n",
    "  FROM Distance_Calculated\n",
    ")\n",
    "\n",
    "SELECT zipcode, Date, Value\n",
    "           FROM ranked_stations R\n",
    "WHERE rank = 1\n",
    "ORDER BY 1,2;\n",
    " \"\"\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " STEP 8:\n",
    " Join purchase_orders_and_invoices, supplier_case, and supplier_zip_code_weather based on zip codes and the transaction date. Only \n",
    " include transactions that have matching temperature readings\n",
    " selecting data from supplier case table, purchase order table and supplier_zip_code_weather \n",
    " and joining them based on zip code and date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting data from supplier case table, purchase order table and supplier_zip_code_weather \n",
    "# and joining them based on zip code and date \n",
    "\n",
    "cs.execute(\"\"\" select s.supplierid,s.suppliername,sz.zipcode,\n",
    "p.orderdate,sz.Value,sz.zipcode\n",
    "from\n",
    "supplier_case s,\n",
    "purchase_orders_and_invoices p,\n",
    "supplier_zip_code_weather_view sz\n",
    "\n",
    "where s.postalpostalcode=sz.zipcode\n",
    "and dateadd(year, +5, p.orderdate)=sz.date \n",
    "           \"\"\")\n",
    "\n",
    "print(cs.fetchall())\n",
    "\n",
    "\n",
    "# Close the connection\n",
    "cs.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
